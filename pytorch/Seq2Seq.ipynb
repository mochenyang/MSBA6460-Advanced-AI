{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aec03969",
   "metadata": {},
   "source": [
    "## Pytorch Implementation of Seq2Seq with RNN (for Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ff4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8ed694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.' 'Go.' 'Go.' 'Go.' 'Hi.' 'Run!' 'Run!' 'Run!' 'Run!' 'Run.']\n",
      "\n",
      "['Ve.' 'Vete.' 'Vaya.' 'Váyase.' 'Hola.' '¡Corre!' '¡Corran!' '¡Corra!'\n",
      " '¡Corred!' 'Corred.']\n",
      "\n",
      "In total: 128084 pairs of sentences.\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset - note that we need to specify encoding=\"utf-8\" when the language contains non ascii words.\n",
    "sentences_english = []\n",
    "sentences_spanish = []\n",
    "for line in open('../datasets/spa.txt', 'r', encoding = 'utf-8'):\n",
    "    s_english, s_spanish, other = line.rstrip('\\n').split('\\t')\n",
    "    sentences_english.append(s_english)\n",
    "    sentences_spanish.append(s_spanish)   \n",
    "\n",
    "sentences_english = np.array(sentences_english)\n",
    "sentences_spanish = np.array(sentences_spanish)\n",
    "# print to check\n",
    "print(sentences_english[0:10])\n",
    "print()\n",
    "print(sentences_spanish[0:10])\n",
    "print()\n",
    "print('In total: ' + str(len(sentences_spanish)) + ' pairs of sentences.')\n",
    "\n",
    "# The original data is quite large, and may result in high memory usage and long training time. Let's take a sample of 15000\n",
    "idx = np.random.choice(list(range(len(sentences_spanish))), size = 15000, replace = False)\n",
    "sentences_english = sentences_english[idx]\n",
    "sentences_spanish = sentences_spanish[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcf61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing and vectorization for English-Spanish sentence pairs\n",
    "TEXT_eng = data.Field(sequential=True, init_token = '<start>', eos_token = '<end>', tokenize='spacy', tokenizer_language='en_core_web_sm', lower=True, batch_first=True)\n",
    "TEXT_spa = data.Field(sequential=True, init_token = '<start>', eos_token = '<end>', tokenize='spacy', tokenizer_language='es_core_news_sm', lower=True, batch_first=True)\n",
    "fields = [('English', TEXT_eng), ('Spanish', TEXT_spa)]\n",
    "examples = []\n",
    "for i in range(len(sentences_english)):\n",
    "    examples.append(data.Example.fromlist([sentences_english[i], sentences_spanish[i]], fields))\n",
    "dataset = data.Dataset(examples, fields)\n",
    "TEXT_eng.build_vocab(dataset)\n",
    "TEXT_spa.build_vocab(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6903057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5816\n",
      "[('.', 12932), ('i', 4398), ('the', 3569), ('to', 3323), ('you', 3221), ('tom', 2550), ('a', 2222), ('?', 2096), ('is', 1846), (\"n't\", 1793)]\n",
      "['<unk>', '<pad>', '<start>', '<end>', '.', 'i', 'the', 'to', 'you', 'tom']\n",
      "9488\n",
      "[('.', 12879), ('de', 2908), ('que', 2706), ('a', 2595), ('no', 2478), ('tom', 2423), ('la', 2205), ('?', 2100), ('¿', 2098), ('el', 2039)]\n",
      "['<unk>', '<pad>', '<start>', '<end>', '.', 'de', 'que', 'a', 'no', 'tom']\n"
     ]
    }
   ],
   "source": [
    "# inspect the vocabulary\n",
    "print(len(TEXT_eng.vocab))\n",
    "print(TEXT_eng.vocab.freqs.most_common(10))\n",
    "print(TEXT_eng.vocab.itos[:10])\n",
    "\n",
    "print(len(TEXT_spa.vocab))\n",
    "print(TEXT_spa.vocab.freqs.most_common(10))\n",
    "print(TEXT_spa.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66c118b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct train_iterator and valid_iterator\n",
    "# each iterator should constain pairs of Enblish sentences and Spanish sentences\n",
    "train_data, valid_data = dataset.split(split_ratio=0.8)\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data), batch_size=128,\n",
    "                                                            sort_key=lambda x: len(x.Spanish),\n",
    "                                                            sort_within_batch=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eae47dca",
   "metadata": {},
   "source": [
    "### Seq2Seq with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2247497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a encoder-decoder model with RNN to translate English to Spanish\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch size, src len]\n",
    "        embedded = self.embedding(src)\n",
    "        # embedded = [batch size, src len, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [batch size, src len, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        return hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        input = input.unsqueeze(1)\n",
    "        # input = [batch size, 1]\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded = [batch size, 1, emb dim]\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output = [batch size, 1, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, trg len]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = len(TEXT_spa.vocab)\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden = self.encoder(src)\n",
    "        # first input to the decoder is the <start> tokens\n",
    "        input = trg[:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden state and the context state\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[:, t, :] = output\n",
    "            # under teacher forcing, use actual next token as next input\n",
    "            input = trg[:, t]\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48cb6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model parameters and training parameters\n",
    "INPUT_DIM = len(TEXT_eng.vocab)\n",
    "OUTPUT_DIM = len(TEXT_spa.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM)\n",
    "model = Seq2Seq(enc, dec)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TEXT_spa.vocab.stoi[TEXT_spa.pad_token])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfa5e184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 5.496793838257485\n",
      "Epoch: 0 Validation Loss: 4.830464164415996\n",
      "Epoch: 1 Loss: 4.3519443095998565\n",
      "Epoch: 1 Validation Loss: 4.229296763737996\n",
      "Epoch: 2 Loss: 3.659259408078295\n",
      "Epoch: 2 Validation Loss: 3.9169470767180123\n",
      "Epoch: 3 Loss: 3.141818868353012\n",
      "Epoch: 3 Validation Loss: 3.706241011619568\n",
      "Epoch: 4 Loss: 2.7004515693542803\n",
      "Epoch: 4 Validation Loss: 3.600416898727417\n",
      "Epoch: 5 Loss: 2.3049231666199703\n",
      "Epoch: 5 Validation Loss: 3.535680582125982\n",
      "Epoch: 6 Loss: 1.9472848227683535\n",
      "Epoch: 6 Validation Loss: 3.503709683815638\n",
      "Epoch: 7 Loss: 1.6276886120755623\n",
      "Epoch: 7 Validation Loss: 3.4893744190533957\n",
      "Epoch: 8 Loss: 1.3456305506381583\n",
      "Epoch: 8 Validation Loss: 3.4803479512532554\n",
      "Epoch: 9 Loss: 1.0968241171633943\n",
      "Epoch: 9 Validation Loss: 3.5109322170416513\n"
     ]
    }
   ],
   "source": [
    "# train the model and print out validation loss after each epoch\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_iterator:\n",
    "        src = batch.English\n",
    "        trg = batch.Spanish\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        # output = [batch size, trg len, output dim]\n",
    "        # trg = [batch size, trg len]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        # output = [batch size * trg len - 1, output dim]\n",
    "        # trg = [batch size * trg len - 1]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('Epoch: ' + str(epoch) + ' Loss: ' + str(epoch_loss / len(train_iterator)))\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iterator:\n",
    "            src = batch.English\n",
    "            trg = batch.Spanish\n",
    "            output = model(src, trg)\n",
    "            # output = [batch size, trg len, output dim]\n",
    "            # trg = [batch size, trg len]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            # output = [batch size * trg len - 1, output dim]\n",
    "            # trg = [batch size * trg len - 1]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    print('Epoch: ' + str(epoch) + ' Validation Loss: ' + str(epoch_loss / len(valid_iterator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bed2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, implement the translate function to translate English to Spanish\n",
    "def translate(sentence, src_field=TEXT_eng, trg_field=TEXT_spa, model=model, max_len=50):\n",
    "    model.eval()\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_tensor)\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]])\n",
    "        # trg_tensor = [1, batch size]\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden)\n",
    "        # output = [batch size, output dim]\n",
    "        pred_token = output.argmax(1).item()\n",
    "        # Notice that because trg_tensor is set to the last element of trg_indexes, in every iteration the predicted token is added back to generate the next prediction\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5487f6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¡', 'hola', 'a', 'todos', '!', '<end>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('Hello world!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e27b390",
   "metadata": {},
   "source": [
    "### Seq2Seq with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64839981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a biderctional RNN encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # typically use a bidirectional RNN for the encoder so that the context vector will be a combination of both forward and backward hidden states\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src = [batch size, src len]\n",
    "        embedded = self.embedding(src)\n",
    "        # embedded = [batch size, src len, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [batch size, src len, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        return outputs, hidden[0, :, :]\n",
    "    \n",
    "# define the attention layer\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        # use hid_dim * 3 because the decoder hidden states and bidirectional encoder hidden states are concatenated together\n",
    "        self.attn = nn.Linear(hid_dim * 3, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, hid dim]\n",
    "        # encoder_outputs = [batch size, src len, hid dim * n directions]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # hidden = [batch size, src len, hid dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy = [batch size, src len, hid dim]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention = [batch size, src len]\n",
    "        return torch.nn.functional.softmax(attention, dim=1)\n",
    "\n",
    "# define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim * 2, hid_dim)\n",
    "        self.fc = nn.Linear(hid_dim * 3 + hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size], input tokens\n",
    "        # hidden = [batch size, hid dim], decoder previous hidden state\n",
    "        # encoder_outputs = [batch size, src len, hid dim * n directions], encoder hidden states\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded = [1, batch size, emb dim]\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a = [batch size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        # a = [batch size, 1, src len]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        # weighted = [batch size, 1, hid dim * n directions]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted = [1, batch size, hid dim * n directions]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input = [1, batch size, emb dim + hid dim * n directions]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [1, batch size, hid dim]\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        prediction = self.fc(torch.cat((output, weighted, hidden), dim=1))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden\n",
    "    \n",
    "# define the seq2seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, trg len]\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        # first input to the decoder is the <start> tokens\n",
    "        input = trg[:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[:, t, :] = output\n",
    "            # under teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[:, t]\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c21e361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model parameters and training parameters\n",
    "INPUT_DIM = len(TEXT_eng.vocab)\n",
    "OUTPUT_DIM = len(TEXT_spa.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ATTN_DIM = 64\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "attn = Attention(HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, attn)\n",
    "model = Seq2Seq(enc, dec)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TEXT_spa.vocab.stoi[TEXT_spa.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2cc47dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 5.000367438539546, Validation Loss: 4.649056712786357\n",
      "Epoch: 1, Training Loss: 3.0213708319562547, Validation Loss: 3.6565954983234406\n",
      "Epoch: 2, Training Loss: 1.6766918897628784, Validation Loss: 3.3978548447291055\n",
      "Epoch: 3, Training Loss: 0.895813929907819, Validation Loss: 3.370510995388031\n",
      "Epoch: 4, Training Loss: 0.5280276555964287, Validation Loss: 3.388024389743805\n",
      "Epoch: 5, Training Loss: 0.30683505899728614, Validation Loss: 3.4587197502454123\n",
      "Epoch: 6, Training Loss: 0.1775913105366078, Validation Loss: 3.526633620262146\n",
      "Epoch: 7, Training Loss: 0.10690247822315135, Validation Loss: 3.5670776764551797\n",
      "Epoch: 8, Training Loss: 0.07229987480380434, Validation Loss: 3.5720036228497825\n",
      "Epoch: 9, Training Loss: 0.049868411780830395, Validation Loss: 3.6083854138851166\n"
     ]
    }
   ],
   "source": [
    "# train the model and print out validation loss after each epoch\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_iterator:\n",
    "        src = batch.English\n",
    "        trg = batch.Spanish\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        # output = [batch size * trg len - 1, output dim]\n",
    "        # trg = [batch size * trg len - 1]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iterator:\n",
    "            src = batch.English\n",
    "            trg = batch.Spanish\n",
    "            output = model(src, trg)\n",
    "            # trg = [trg len, batch size]\n",
    "            # output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg len - 1) * batch size]\n",
    "            # output = [(trg len - 1) * batch size, output dim]\n",
    "            loss = criterion(output, trg)\n",
    "            valid_loss += loss.item()\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss / len(train_iterator), valid_loss / len(valid_iterator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "582a805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, implement the translate function to translate English to Spanish\n",
    "def translate(sentence, src_field=TEXT_eng, trg_field=TEXT_spa, model=model, max_len=50):\n",
    "    model.eval()\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0)\n",
    "    # src_tensor = [batch size, src len]\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    # encoder_outputs = [batch size, src len, hid dim * n directions]\n",
    "    # hidden = [batch size, hid dim]\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]])\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "        # output = [batch size, output dim]\n",
    "        # hidden = [batch size, hid dim]\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3af95587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¡', 'mundo', '!', '<end>']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('Hello world!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
