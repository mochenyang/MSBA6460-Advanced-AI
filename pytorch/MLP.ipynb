{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec03969",
   "metadata": {},
   "source": [
    "## Pytorch Implementation of Feed-Forward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ff4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f054a302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (45, 4) (105, 3) (45, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "x = []\n",
    "y = []\n",
    "for line in open(\"../datasets/iris.csv\"):\n",
    "    data = line.rstrip('\\n').split(',')\n",
    "    x.append(data[0:4])\n",
    "    y.append(data[4])\n",
    "\n",
    "# convert feature lists to a numpy array\n",
    "x = np.array(x, dtype = float)\n",
    "# one-hot encode the label (3 classes) - a manual approach\n",
    "label = np.zeros((len(y), 3))\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'Iris-setosa':\n",
    "        label[i][0] = 1\n",
    "    if y[i] == 'Iris-versicolor':\n",
    "        label[i][1] = 1\n",
    "    if y[i] == 'Iris-virginica':\n",
    "        label[i][2] = 1\n",
    "label = np.array(label, dtype = float)\n",
    "\n",
    "# training-validation split (70% training)\n",
    "x_train, x_val, label_train, label_val = train_test_split(x, label, test_size = 0.3)\n",
    "print(x_train.shape, x_val.shape, label_train.shape, label_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee4a7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataLoader objects to read data for pytorch\n",
    "# The following DataLoader takes numpy array as inputs\n",
    "class IrisData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype = torch.float32)\n",
    "        self.y = torch.tensor(y, dtype = torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9da964e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(IrisData(x_train, label_train), shuffle = True, batch_size = 32)\n",
    "val_loader = DataLoader(IrisData(x_val, label_val), shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e1cbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, define the feed-forward neural network architecture\n",
    "class FFNet(nn.Module):\n",
    "    # first need to implement the init that specifies all layers\n",
    "    def __init__(self):\n",
    "        super(FFNet, self).__init__()\n",
    "        # Relu hidden layer that takes 4 features and have 8 hidden units\n",
    "        self.l1 = nn.Linear(4,8)\n",
    "        self.relu = nn.ReLU()\n",
    "        # then the output layer before softmax\n",
    "        # softmax is applied as part of loss function\n",
    "        self.l2 = nn.Linear(8,3)\n",
    "    \n",
    "    # next need to implement a forward method that takes an input through the network\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81e427d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model and set training parameters\n",
    "model = FFNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71d8d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 training loss =  tensor(1.0274, grad_fn=<DivBackward1>) val loss =  tensor(0.4749)\n",
      "Epoch: 1 training loss =  tensor(1.3813, grad_fn=<DivBackward1>) val loss =  tensor(0.6420)\n",
      "Epoch: 2 training loss =  tensor(1.0155, grad_fn=<DivBackward1>) val loss =  tensor(0.7456)\n",
      "Epoch: 3 training loss =  tensor(0.9442, grad_fn=<DivBackward1>) val loss =  tensor(0.7734)\n",
      "Epoch: 4 training loss =  tensor(0.9235, grad_fn=<DivBackward1>) val loss =  tensor(0.8066)\n",
      "Epoch: 5 training loss =  tensor(1.0432, grad_fn=<DivBackward1>) val loss =  tensor(0.7893)\n",
      "Epoch: 6 training loss =  tensor(0.8383, grad_fn=<DivBackward1>) val loss =  tensor(0.7727)\n",
      "Epoch: 7 training loss =  tensor(0.9549, grad_fn=<DivBackward1>) val loss =  tensor(0.7508)\n",
      "Epoch: 8 training loss =  tensor(0.9607, grad_fn=<DivBackward1>) val loss =  tensor(0.7458)\n",
      "Epoch: 9 training loss =  tensor(0.9344, grad_fn=<DivBackward1>) val loss =  tensor(0.7330)\n",
      "Epoch: 10 training loss =  tensor(1.0057, grad_fn=<DivBackward1>) val loss =  tensor(0.7240)\n",
      "Epoch: 11 training loss =  tensor(0.7930, grad_fn=<DivBackward1>) val loss =  tensor(0.6707)\n",
      "Epoch: 12 training loss =  tensor(0.8615, grad_fn=<DivBackward1>) val loss =  tensor(0.6557)\n",
      "Epoch: 13 training loss =  tensor(0.8170, grad_fn=<DivBackward1>) val loss =  tensor(0.6290)\n",
      "Epoch: 14 training loss =  tensor(0.7674, grad_fn=<DivBackward1>) val loss =  tensor(0.6162)\n",
      "Epoch: 15 training loss =  tensor(0.8335, grad_fn=<DivBackward1>) val loss =  tensor(0.6321)\n",
      "Epoch: 16 training loss =  tensor(0.8435, grad_fn=<DivBackward1>) val loss =  tensor(0.6071)\n",
      "Epoch: 17 training loss =  tensor(0.7461, grad_fn=<DivBackward1>) val loss =  tensor(0.5809)\n",
      "Epoch: 18 training loss =  tensor(0.7130, grad_fn=<DivBackward1>) val loss =  tensor(0.5534)\n",
      "Epoch: 19 training loss =  tensor(0.8902, grad_fn=<DivBackward1>) val loss =  tensor(0.5777)\n",
      "Epoch: 20 training loss =  tensor(0.7958, grad_fn=<DivBackward1>) val loss =  tensor(0.5450)\n",
      "Epoch: 21 training loss =  tensor(0.7412, grad_fn=<DivBackward1>) val loss =  tensor(0.5225)\n",
      "Epoch: 22 training loss =  tensor(0.8373, grad_fn=<DivBackward1>) val loss =  tensor(0.5244)\n",
      "Epoch: 23 training loss =  tensor(0.7053, grad_fn=<DivBackward1>) val loss =  tensor(0.5013)\n",
      "Epoch: 24 training loss =  tensor(0.7691, grad_fn=<DivBackward1>) val loss =  tensor(0.5053)\n",
      "Epoch: 25 training loss =  tensor(0.7815, grad_fn=<DivBackward1>) val loss =  tensor(0.5009)\n",
      "Epoch: 26 training loss =  tensor(0.5779, grad_fn=<DivBackward1>) val loss =  tensor(0.4513)\n",
      "Epoch: 27 training loss =  tensor(0.7553, grad_fn=<DivBackward1>) val loss =  tensor(0.4318)\n",
      "Epoch: 28 training loss =  tensor(0.6886, grad_fn=<DivBackward1>) val loss =  tensor(0.4160)\n",
      "Epoch: 29 training loss =  tensor(0.6147, grad_fn=<DivBackward1>) val loss =  tensor(0.3912)\n",
      "Epoch: 30 training loss =  tensor(0.6619, grad_fn=<DivBackward1>) val loss =  tensor(0.3936)\n",
      "Epoch: 31 training loss =  tensor(0.7045, grad_fn=<DivBackward1>) val loss =  tensor(0.3923)\n",
      "Epoch: 32 training loss =  tensor(0.7075, grad_fn=<DivBackward1>) val loss =  tensor(0.3815)\n",
      "Epoch: 33 training loss =  tensor(0.6528, grad_fn=<DivBackward1>) val loss =  tensor(0.3738)\n",
      "Epoch: 34 training loss =  tensor(0.4959, grad_fn=<DivBackward1>) val loss =  tensor(0.3504)\n",
      "Epoch: 35 training loss =  tensor(0.7039, grad_fn=<DivBackward1>) val loss =  tensor(0.3476)\n",
      "Epoch: 36 training loss =  tensor(0.6879, grad_fn=<DivBackward1>) val loss =  tensor(0.3415)\n",
      "Epoch: 37 training loss =  tensor(0.5539, grad_fn=<DivBackward1>) val loss =  tensor(0.3292)\n",
      "Epoch: 38 training loss =  tensor(0.5333, grad_fn=<DivBackward1>) val loss =  tensor(0.3223)\n",
      "Epoch: 39 training loss =  tensor(0.5483, grad_fn=<DivBackward1>) val loss =  tensor(0.3036)\n",
      "Epoch: 40 training loss =  tensor(0.5984, grad_fn=<DivBackward1>) val loss =  tensor(0.3011)\n",
      "Epoch: 41 training loss =  tensor(0.7630, grad_fn=<DivBackward1>) val loss =  tensor(0.3035)\n",
      "Epoch: 42 training loss =  tensor(0.4942, grad_fn=<DivBackward1>) val loss =  tensor(0.2761)\n",
      "Epoch: 43 training loss =  tensor(0.4409, grad_fn=<DivBackward1>) val loss =  tensor(0.2629)\n",
      "Epoch: 44 training loss =  tensor(0.5926, grad_fn=<DivBackward1>) val loss =  tensor(0.2650)\n",
      "Epoch: 45 training loss =  tensor(0.5050, grad_fn=<DivBackward1>) val loss =  tensor(0.2583)\n",
      "Epoch: 46 training loss =  tensor(0.5803, grad_fn=<DivBackward1>) val loss =  tensor(0.2535)\n",
      "Epoch: 47 training loss =  tensor(0.3741, grad_fn=<DivBackward1>) val loss =  tensor(0.2373)\n",
      "Epoch: 48 training loss =  tensor(0.6639, grad_fn=<DivBackward1>) val loss =  tensor(0.2360)\n",
      "Epoch: 49 training loss =  tensor(0.4803, grad_fn=<DivBackward1>) val loss =  tensor(0.2258)\n",
      "Epoch: 50 training loss =  tensor(0.7327, grad_fn=<DivBackward1>) val loss =  tensor(0.2338)\n",
      "Epoch: 51 training loss =  tensor(0.4132, grad_fn=<DivBackward1>) val loss =  tensor(0.2170)\n",
      "Epoch: 52 training loss =  tensor(0.3877, grad_fn=<DivBackward1>) val loss =  tensor(0.2072)\n",
      "Epoch: 53 training loss =  tensor(0.5453, grad_fn=<DivBackward1>) val loss =  tensor(0.2122)\n",
      "Epoch: 54 training loss =  tensor(0.4729, grad_fn=<DivBackward1>) val loss =  tensor(0.2034)\n",
      "Epoch: 55 training loss =  tensor(0.3989, grad_fn=<DivBackward1>) val loss =  tensor(0.1957)\n",
      "Epoch: 56 training loss =  tensor(0.4583, grad_fn=<DivBackward1>) val loss =  tensor(0.1956)\n",
      "Epoch: 57 training loss =  tensor(0.6073, grad_fn=<DivBackward1>) val loss =  tensor(0.1941)\n",
      "Epoch: 58 training loss =  tensor(0.2402, grad_fn=<DivBackward1>) val loss =  tensor(0.1787)\n",
      "Epoch: 59 training loss =  tensor(0.5556, grad_fn=<DivBackward1>) val loss =  tensor(0.1837)\n",
      "Epoch: 60 training loss =  tensor(0.3995, grad_fn=<DivBackward1>) val loss =  tensor(0.1783)\n",
      "Epoch: 61 training loss =  tensor(0.3336, grad_fn=<DivBackward1>) val loss =  tensor(0.1719)\n",
      "Epoch: 62 training loss =  tensor(0.5791, grad_fn=<DivBackward1>) val loss =  tensor(0.1770)\n",
      "Epoch: 63 training loss =  tensor(0.5204, grad_fn=<DivBackward1>) val loss =  tensor(0.1759)\n",
      "Epoch: 64 training loss =  tensor(0.4064, grad_fn=<DivBackward1>) val loss =  tensor(0.1662)\n",
      "Epoch: 65 training loss =  tensor(0.5649, grad_fn=<DivBackward1>) val loss =  tensor(0.1688)\n",
      "Epoch: 66 training loss =  tensor(0.5350, grad_fn=<DivBackward1>) val loss =  tensor(0.1615)\n",
      "Epoch: 67 training loss =  tensor(0.5533, grad_fn=<DivBackward1>) val loss =  tensor(0.1595)\n",
      "Epoch: 68 training loss =  tensor(0.3519, grad_fn=<DivBackward1>) val loss =  tensor(0.1555)\n",
      "Epoch: 69 training loss =  tensor(0.4244, grad_fn=<DivBackward1>) val loss =  tensor(0.1552)\n",
      "Epoch: 70 training loss =  tensor(0.6245, grad_fn=<DivBackward1>) val loss =  tensor(0.1577)\n",
      "Epoch: 71 training loss =  tensor(0.4425, grad_fn=<DivBackward1>) val loss =  tensor(0.1541)\n",
      "Epoch: 72 training loss =  tensor(0.5393, grad_fn=<DivBackward1>) val loss =  tensor(0.1513)\n",
      "Epoch: 73 training loss =  tensor(0.2815, grad_fn=<DivBackward1>) val loss =  tensor(0.1397)\n",
      "Epoch: 74 training loss =  tensor(0.4393, grad_fn=<DivBackward1>) val loss =  tensor(0.1381)\n",
      "Epoch: 75 training loss =  tensor(0.3943, grad_fn=<DivBackward1>) val loss =  tensor(0.1349)\n",
      "Epoch: 76 training loss =  tensor(0.4530, grad_fn=<DivBackward1>) val loss =  tensor(0.1355)\n",
      "Epoch: 77 training loss =  tensor(0.4175, grad_fn=<DivBackward1>) val loss =  tensor(0.1313)\n",
      "Epoch: 78 training loss =  tensor(0.6783, grad_fn=<DivBackward1>) val loss =  tensor(0.1369)\n",
      "Epoch: 79 training loss =  tensor(0.3133, grad_fn=<DivBackward1>) val loss =  tensor(0.1298)\n",
      "Epoch: 80 training loss =  tensor(0.3899, grad_fn=<DivBackward1>) val loss =  tensor(0.1280)\n",
      "Epoch: 81 training loss =  tensor(0.5332, grad_fn=<DivBackward1>) val loss =  tensor(0.1292)\n",
      "Epoch: 82 training loss =  tensor(0.4933, grad_fn=<DivBackward1>) val loss =  tensor(0.1253)\n",
      "Epoch: 83 training loss =  tensor(0.3671, grad_fn=<DivBackward1>) val loss =  tensor(0.1192)\n",
      "Epoch: 84 training loss =  tensor(0.3934, grad_fn=<DivBackward1>) val loss =  tensor(0.1209)\n",
      "Epoch: 85 training loss =  tensor(0.4604, grad_fn=<DivBackward1>) val loss =  tensor(0.1188)\n",
      "Epoch: 86 training loss =  tensor(0.3529, grad_fn=<DivBackward1>) val loss =  tensor(0.1141)\n",
      "Epoch: 87 training loss =  tensor(0.4871, grad_fn=<DivBackward1>) val loss =  tensor(0.1172)\n",
      "Epoch: 88 training loss =  tensor(0.4595, grad_fn=<DivBackward1>) val loss =  tensor(0.1145)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 training loss =  tensor(0.4602, grad_fn=<DivBackward1>) val loss =  tensor(0.1145)\n",
      "Epoch: 90 training loss =  tensor(0.2790, grad_fn=<DivBackward1>) val loss =  tensor(0.1100)\n",
      "Epoch: 91 training loss =  tensor(0.2854, grad_fn=<DivBackward1>) val loss =  tensor(0.1048)\n",
      "Epoch: 92 training loss =  tensor(0.4526, grad_fn=<DivBackward1>) val loss =  tensor(0.1043)\n",
      "Epoch: 93 training loss =  tensor(0.4199, grad_fn=<DivBackward1>) val loss =  tensor(0.1047)\n",
      "Epoch: 94 training loss =  tensor(0.4377, grad_fn=<DivBackward1>) val loss =  tensor(0.1075)\n",
      "Epoch: 95 training loss =  tensor(0.3346, grad_fn=<DivBackward1>) val loss =  tensor(0.1056)\n",
      "Epoch: 96 training loss =  tensor(0.5178, grad_fn=<DivBackward1>) val loss =  tensor(0.1048)\n",
      "Epoch: 97 training loss =  tensor(0.5123, grad_fn=<DivBackward1>) val loss =  tensor(0.1075)\n",
      "Epoch: 98 training loss =  tensor(0.3799, grad_fn=<DivBackward1>) val loss =  tensor(0.1038)\n",
      "Epoch: 99 training loss =  tensor(0.2889, grad_fn=<DivBackward1>) val loss =  tensor(0.0980)\n"
     ]
    }
   ],
   "source": [
    "# start training:\n",
    "for epoch in range(epochs):\n",
    "    # training mode\n",
    "    model.train()\n",
    "    for batch_id, (x_train, y_train) in enumerate(train_loader):\n",
    "        y_pred = model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # validation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x_val, y_val) in val_loader:\n",
    "            y_pred = model(x_val)\n",
    "            val_loss = criterion(y_pred, y_val)\n",
    "    \n",
    "    # print training and val loss\n",
    "    print(\"Epoch:\", epoch, \"training loss = \", train_loss, \"val loss = \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc36a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9423, 0.0541, 0.0036], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "input = torch.tensor([5.1,3.5,1.4,0.2])\n",
    "raw = model(input)\n",
    "# get predicted probs\n",
    "nn.Softmax(dim = 0)(raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
